{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2021/tavg-202105-cty-scaled.csv\n",
    "import urllib.request\n",
    "\n",
    "url = ['https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2021/tavg-202105-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2021/tavg-202106-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2021/tavg-202107-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2021/tavg-202108-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2021/tavg-202109-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2021/tavg-202110-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2021/tavg-202111-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2021/tavg-202112-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202201-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202202-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202203-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202204-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202205-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202206-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202207-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202208-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202209-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202210-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202211-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2022/tavg-202212-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2023/tavg-202301-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2023/tavg-202302-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2023/tavg-202303-cty-scaled.csv',\n",
    "       'https://www.ncei.noaa.gov/data/nclimgrid-daily/access/averages/2023/tavg-202304-cty-scaled.csv'       \n",
    "      ]\n",
    "filepath = 'E:\\\\Google_Heat\\\\temprature\\\\'\n",
    "\n",
    "#for i in url:\n",
    "#    urllib.request.urlretrieve(i, filepath+i.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "colnames=['cty','code','sub_region_12','year','month','TAVG','1',\n",
    " '2',\n",
    " '3',\n",
    " '4',\n",
    " '5',\n",
    " '6',\n",
    " '7',\n",
    " '8',\n",
    " '9',\n",
    " '10',\n",
    " '11',\n",
    " '12',\n",
    " '13',\n",
    " '14',\n",
    " '15',\n",
    " '16',\n",
    " '17',\n",
    " '18',\n",
    " '19',\n",
    " '20',\n",
    " '21',\n",
    " '22',\n",
    " '23',\n",
    " '24',\n",
    " '25',\n",
    " '26',\n",
    " '27',\n",
    " '28',\n",
    " '29',\n",
    " '30',\n",
    " '31']\n",
    "df_daily_average_T=pd.read_csv(filepath+'tavg-202105-cty-scaled.csv', sep=\",\", names=colnames, header=None)\n",
    "df_daily_average_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b78e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_average_T=pd.read_csv(filepath+'tavg-202105-cty-scaled.csv', sep=\",\",header=None)\n",
    "df_daily_average_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from datetime import datetime\n",
    "\n",
    "start_date = datetime(2021, 5, 1)\n",
    "end_date = datetime(2023, 4, 30)\n",
    "ldays_in_month=[]\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    year = current_date.year\n",
    "    month = current_date.month\n",
    "    days_in_month = calendar.monthrange(year, month)[1]\n",
    "    ldays_in_month.append(days_in_month)\n",
    "    print(f\"{calendar.month_name[month]} {year}: {days_in_month} days\")\n",
    "    current_date = current_date.replace(day=1) + relativedelta(months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get the file paths of all CSV files in the directory\n",
    "file_paths = glob.glob('E:\\\\Google_Heat\\\\temprature\\\\*.csv')\n",
    "\n",
    "# Create an empty list to store the DataFrames for each file\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file path\n",
    "nn=0\n",
    "for file_path in file_paths:\n",
    "    # Read the CSV file and extract columns 6 to 36\n",
    "    df = pd.read_csv(file_path, header=None, usecols=range(6, 6+ldays_in_month[nn]))\n",
    "    nn=nn+1\n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate the DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dfs, axis=1)\n",
    "\n",
    "# Print the merged DataFrame\n",
    "#print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25792091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime(2021, 5, 1)\n",
    "end_date = datetime(2023, 4, 30)\n",
    "\n",
    "# Create an empty list to store the dates\n",
    "date_list = []\n",
    "\n",
    "# Loop through each date in the range\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    date_list.append(current_date)\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "date_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df = merged_df.drop(merged_df.columns[merged_df.eq(-999.99).all()], axis=1)\n",
    "merged_df.columns = date_list\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ab1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a merged DataFrame called 'merged_df'\n",
    "\n",
    "# Select a row from the merged DataFrame\n",
    "row_index = 10  # Specify the row index you want to plot\n",
    "row_data = merged_df.iloc[row_index]\n",
    "\n",
    "# Plot the row data\n",
    "plt.plot(row_data)\n",
    "plt.xlabel('Column Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Plot of Row {row_index}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77234e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_add = df_daily_average_T.iloc[:, 2].rename('sub_region_2')\n",
    "\n",
    "column_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_with_column = pd.concat([merged_df, column_to_add], axis=1)\n",
    "merged_df_with_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde40850",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_to_add = df_daily_average_T.iloc[:, 1].rename('sub_region_2_code')\n",
    "\n",
    "merged_df_with_column = pd.concat([merged_df_with_column, column_to_add], axis=1)\n",
    "merged_df_with_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddc16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df_with_column['sub_region_2'].str.replace(' County', '').nunique()\n",
    "merged_df_with_column['sub_region_2']=merged_df_with_column['sub_region_2'].str.replace(' County', '')\n",
    "merged_df_with_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_with_column.to_csv('AVRG_T.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_US_county_heat_insights=pd.read_csv('E:\\\\Google_Heat\\\\Dataset_to_share\\\\Copy of daily_US_county_heat_insights.csv', sep=\",\")\n",
    "df_daily_US_county_heat_insights['date'] = pd.to_datetime(df_daily_US_county_heat_insights['date'])\n",
    "\n",
    "# Mapping dictionary for state abbreviations\n",
    "state_abbreviations = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
    "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n",
    "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n",
    "    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n",
    "    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n",
    "    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "# Convert state names to abbreviations\n",
    "df_daily_US_county_heat_insights['sub_region_1'] = df_daily_US_county_heat_insights['sub_region_1'].map(state_abbreviations)\n",
    "\n",
    "df_daily_US_county_heat_insights['sub_region_2']=df_daily_US_county_heat_insights['sub_region_1']+': '+df_daily_US_county_heat_insights['sub_region_2'].str.replace(' County', '')\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_daily_US_county_heat_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24007dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatall0=df_daily_US_county_heat_insights[df_daily_US_county_heat_insights['sub_region_2']==merged_df_with_column['sub_region_2'][0]]['heat_stroke_and_exhaustion'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a merged DataFrame called 'merged_df'\n",
    "\n",
    "# Select a row from the merged DataFrame\n",
    "row_index = 0  # Specify the row index you want to plot\n",
    "row_data = merged_df.iloc[row_index]\n",
    "\n",
    "\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have two time series data: time_series1 and time_series2\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 6))\n",
    "\n",
    "# Plot time_series1 in the first subplot\n",
    "ax1.plot(row_data)\n",
    "ax1.set_ylabel('temp')\n",
    "\n",
    "# Plot time_series2 in the second subplot\n",
    "ax2.plot(heatall0)\n",
    "ax2.set_ylabel('heat')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f316e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a merged DataFrame called 'merged_df'\n",
    "desired_row = merged_df_with_column[merged_df_with_column['sub_region_2'] == 'TX: Travis'].index.values[0]\n",
    "\n",
    "row_data = merged_df.iloc[desired_row]\n",
    "\n",
    "\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have two time series data: time_series1 and time_series2\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 6))\n",
    "\n",
    "# Plot time_series1 in the first subplot\n",
    "ax1.plot(row_data)\n",
    "ax1.set_ylabel('temp')\n",
    "\n",
    "heatall0=df_daily_US_county_heat_insights[df_daily_US_county_heat_insights['sub_region_2']==merged_df_with_column['sub_region_2'][desired_row]]['heat_stroke_and_exhaustion'].values\n",
    "dates=df_daily_US_county_heat_insights[df_daily_US_county_heat_insights['sub_region_2']==merged_df_with_column['sub_region_2'][desired_row]]['date'].values\n",
    "\n",
    "ax2.plot(dates,heatall0)\n",
    "ax2.set_ylabel('heat')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18133921",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_US_county_heat_insights['sub_region_2'].nunique(),merged_df_with_column['sub_region_2'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatall0=df_daily_US_county_heat_insights[df_daily_US_county_heat_insights['sub_region_2']==merged_df_with_column['sub_region_2'][110]]['heat_stroke_and_exhaustion'].values\n",
    "dates=df_daily_US_county_heat_insights[df_daily_US_county_heat_insights['sub_region_2']==merged_df_with_column['sub_region_2'][110]]['date'].values\n",
    "len(dates),len(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_Counties_Heat = pd.DataFrame(columns=range(0, 730))\n",
    "\n",
    "# Convert column names to dates\n",
    "start_date = pd.to_datetime('2021-05-01')\n",
    "end_date = start_date + pd.DateOffset(days=729)\n",
    "column_dates = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "# Assign the column dates to the DataFrame\n",
    "US_Counties_Heat.columns = column_dates\n",
    "US_Counties_Heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcountiesinheat=df_daily_US_county_heat_insights['sub_region_2'].unique()\n",
    "for i in lcountiesinheat:\n",
    "    heat_list = df_daily_US_county_heat_insights[df_daily_US_county_heat_insights['sub_region_2']==i]['heat_stroke_and_exhaustion'].values\n",
    "    dates_list = df_daily_US_county_heat_insights[df_daily_US_county_heat_insights['sub_region_2']==i]['date'].values\n",
    "    \n",
    "    US_Counties_Heat.loc[len(US_Counties_Heat)] = pd.Series(heat_list, index=dates_list)\n",
    "\n",
    "US_Counties_Heat['sub_region_2']= lcountiesinheat\n",
    "US_Counties_Heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a202f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_US_county_heat_insights['sub_region_2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22be539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#desired_row = US_Counties_Heat[US_Counties_Heat['sub_region_2'] == 'TX: Travis'].index.values[0]\n",
    "\n",
    "row_data = US_Counties_Heat.iloc[0]\n",
    "\n",
    "\n",
    "plt.plot(row_data.values[0:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e6ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_data.values[0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fef4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_with_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_Counties_Heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fb189",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = merged_df_with_column.columns.intersection(US_Counties_Heat.columns)\n",
    "\n",
    "# Create a new DataFrame with the common columns\n",
    "new_dataframe = merged_df_with_column.loc[:, common_columns]\n",
    "new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad378951",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_region_2_values = US_Counties_Heat['sub_region_2'].unique()\n",
    "\n",
    "filtered_new_dataframe = new_dataframe[new_dataframe['sub_region_2'].isin(sub_region_2_values)]\n",
    "filtered_new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(\"E:\\\\Google_Heat\\\\uscounties.csv\", sep=\",\")\n",
    "\n",
    "df0.rename(columns={'county_full': 'sub_region_2'}, inplace=True)\n",
    "df0.rename(columns={'state_name': 'sub_region_1'}, inplace=True)\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['sub_region_2']=df0['state_id']+': '+df0['sub_region_2'].str.replace(' County', '')\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ff57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_lat_lon_T = merged_df_with_column.merge(df0[['sub_region_2', 'lat', 'lng']], on=['sub_region_2'], how='left')\n",
    "merged_df_lat_lon_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import glob\n",
    "import os\n",
    "\n",
    "lframes = []\n",
    "for idf in range(0, 730):\n",
    "    frame_data = go.Scatter(\n",
    "        x=merged_df_lat_lon_T['lng'],\n",
    "        y=merged_df_lat_lon_T['lat'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color=merged_df_lat_lon_T[merged_df_lat_lon_T.columns[idf]],\n",
    "            colorscale='Viridis',\n",
    "            opacity=0.4,\n",
    "            colorbar=dict(thickness=10, orientation='h')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    annotation = dict(\n",
    "        text=f\"Date: {merged_df_lat_lon_T.columns[idf]}\",\n",
    "        showarrow=False,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        x=0.05,\n",
    "        y=0.05,\n",
    "        bgcolor=\"white\",\n",
    "        opacity=0.8\n",
    "    )\n",
    "\n",
    "    lframes.append(go.Frame(data=[frame_data], name=f\"Date: {merged_df_lat_lon_T.columns[idf]}\", layout=go.Layout(annotations=[annotation])))\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter(\n",
    "        x=merged_df_lat_lon_T['lng'],\n",
    "        y=merged_df_lat_lon_T['lat'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            symbol='square',\n",
    "            size=6,\n",
    "            color=merged_df_lat_lon_T[merged_df_lat_lon_T.columns[0]],\n",
    "            colorscale='Viridis',\n",
    "            opacity=0.4,\n",
    "            colorbar=dict(thickness=10, orientation='h')\n",
    "        )\n",
    "    )],\n",
    "    layout=go.Layout(\n",
    "        xaxis=dict(range=[min(merged_df_lat_lon_T['lng']), max(merged_df_lat_lon_T['lat'])], autorange=True),\n",
    "        yaxis=dict(range=[min(merged_df_lat_lon_T['lng']), max(merged_df_lat_lon_T['lat'])], autorange=True),\n",
    "        sliders=[dict(\n",
    "            active=0,\n",
    "            currentvalue=dict(\n",
    "                prefix=\"Frame: \",\n",
    "                font=dict(color=\"#666\"),\n",
    "                xanchor=\"left\",\n",
    "                visible=True,\n",
    "                offset=10\n",
    "            ),\n",
    "            pad=dict(t=50),\n",
    "            len=0.9,\n",
    "            x=0.1,\n",
    "            y=0,\n",
    "            steps=[dict(\n",
    "                method=\"animate\",\n",
    "                args=[[f\"Date: {merged_df_lat_lon_T.columns[idf]}\"], {\"frame\": {\"duration\": 0, \"redraw\": False}, \"mode\": \"immediate\"}],\n",
    "                label=f\"Date: {merged_df_lat_lon_T.columns[idf]}\"\n",
    "            ) for idf in range(365)]\n",
    "        )],\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                type=\"buttons\",\n",
    "                buttons=[\n",
    "                    dict(\n",
    "                        label=\"Play\",\n",
    "                        method=\"animate\",\n",
    "                        args=[None]\n",
    "                    ),\n",
    "                    dict(\n",
    "                        label=\"Pause\",\n",
    "                        method=\"animate\",\n",
    "                        args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False}, \"mode\": \"immediate\"}]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    frames=lframes\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1100, \n",
    "    height=800  \n",
    ")\n",
    "fig.write_html('us_heat_illness.html')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a merged DataFrame called 'merged_df'\n",
    "desired_row = merged_df_with_column[merged_df_with_column['sub_region_2'] == 'TX: Travis'].index.values[0]\n",
    "\n",
    "row_data = merged_df.iloc[desired_row]\n",
    "\n",
    "\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have two time series data: time_series1 and time_series2\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 6))\n",
    "\n",
    "# Plot time_series1 in the first subplot\n",
    "ax1.plot(row_data)\n",
    "ax1.set_ylabel('temp')\n",
    "\n",
    "heatall0=df_daily_US_county_heat_insights[df_daily_US_county_heat_insights['sub_region_2']==merged_df_with_column['sub_region_2'][desired_row]]['heat_stroke_and_exhaustion'].values\n",
    "dates=df_daily_US_county_heat_insights[df_daily_US_county_heat_insights['sub_region_2']==merged_df_with_column['sub_region_2'][desired_row]]['date'].values\n",
    "\n",
    "ax2.plot(dates,heatall0)\n",
    "ax2.set_ylabel('heat')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(512)\n",
    "y = row_data\n",
    "coef1, freqs2=pywt.cwt(y,np.arange(1,120),'morl')\n",
    "plt.matshow(coef1) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(512)\n",
    "y = heatall0\n",
    "coef2, freqs2=pywt.cwt(y,np.arange(1,120),'morl')\n",
    "plt.matshow(coef2) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Cross Wavelet Transform (XWT) by element-wise multiplication\n",
    "xwt_coeffs = coef1 * np.conj(coef2)\n",
    "\n",
    "# Calculate the Wavelet Power Spectrum (WPS) from the XWT\n",
    "wps = np.abs(xwt_coeffs) ** 2\n",
    "\n",
    "# Plot the XWT and WPS\n",
    "plt.matshow(wps)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073baf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "lat = merged_df_lat_lon_T['lat'].values # Latitudes of data points\n",
    "lon = merged_df_lat_lon_T['lng'].values  # Longitudes of data points\n",
    "values = merged_df_lat_lon_T[merged_df_lat_lon_T.columns[10]].values    # Values of data points\n",
    "nan_indexes = np.isnan(lat) | np.isnan(lon) | np.isnan(values)\n",
    "\n",
    "# Remove the corresponding indexes from lat, lon, and values arrays\n",
    "lat = lat[~nan_indexes]\n",
    "lon = lon[~nan_indexes]\n",
    "values = values[~nan_indexes]\n",
    "#values=np.nan_to_num(values, nan=0)\n",
    "# Define the desired grid size and range\n",
    "grid_size = 0.2  # Specify the grid size (adjust as needed)\n",
    "min_lat = min(lat) - grid_size  # Minimum latitude minus grid size\n",
    "max_lat = max(lat) + grid_size  # Maximum latitude plus grid size\n",
    "min_lon = min(lon) - grid_size  # Minimum longitude minus grid size\n",
    "max_lon = max(lon) + grid_size  # Maximum longitude plus grid size\n",
    "\n",
    "grid_lat, grid_lon = np.mgrid[min_lat:max_lat:grid_size, min_lon:max_lon:grid_size]\n",
    "\n",
    "grid_values = np.full(grid_lat.shape, np.nan)\n",
    "\n",
    "for i in range(grid_lat.shape[0]):\n",
    "    for j in range(grid_lat.shape[1]):\n",
    "        grid_lat_center = grid_lat[i, j]\n",
    "        grid_lon_center = grid_lon[i, j]\n",
    "        distances = np.sqrt((lat - grid_lat_center) ** 2 + (lon - grid_lon_center) ** 2)\n",
    "        closest_index = np.argmin(distances)\n",
    "       # print(closest_index)\n",
    "        grid_values[i, j] = values[closest_index]\n",
    "\n",
    "print(grid_lat)\n",
    "print(grid_lon)\n",
    "print(grid_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the grid_values array\n",
    "plt.figure(figsize=(15,8))\n",
    "# Plot the grid_values as a heatmap\n",
    "plt.imshow(np.rot90(np.rot90(grid_values)), cmap='viridis', extent=[min_lon, max_lon, min_lat, max_lat])\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Grid Values Map')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Assuming you have the grid_values, grid_lat, and grid_lon arrays\n",
    "\n",
    "# Define the exact US mainland boundary polygon using its coordinates\n",
    "us_mainland_boundary = us49.boundary\n",
    "\n",
    "# Iterate over each grid point and check if it falls outside the US mainland boundary\n",
    "for i in range(grid_lat.shape[0]):\n",
    "    for j in range(grid_lat.shape[1]):\n",
    "        point = Point(grid_lon[i, j], grid_lat[i, j])\n",
    "        if not us_mainland_boundary.contains(point):\n",
    "            grid_values[i, j] = np.nan\n",
    "            grid_lat[i, j] = np.nan\n",
    "            grid_lon[i, j] = np.nan\n",
    "\n",
    "# Flatten the grid_lat, grid_lon, and grid_values arrays\n",
    "x = grid_lon.flatten()\n",
    "y = grid_lat.flatten()\n",
    "z = grid_values.flatten()\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y, c=z, cmap='viridis', alpha=0.5)\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Scatter Plot of Grid Values')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1512ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import contextily as ctx \n",
    "import geopandas as gpd \n",
    "import os \n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "path = \"tl_2022_us_state.shp\"\n",
    "df = gpd.read_file(path)\n",
    "df = df.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_continental = ['HI','VI','MP','GU','AK','AS','PR']\n",
    "us49 = df\n",
    "for n in non_continental:\n",
    "    us49 = us49[us49.STUSPS != n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26054c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "us49.boundary.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de436af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "path = \"tl_2022_us_state.shp\"\n",
    "df = gpd.read_file(path)\n",
    "df = df.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Extract the latitude and longitude coordinates\n",
    "boundary_lat = []\n",
    "boundary_lon = []\n",
    "\n",
    "for geometry in df['geometry']:\n",
    "    if geometry.geom_type == 'Polygon':\n",
    "        boundary_lat.append(geometry.exterior.coords.xy[1])\n",
    "        boundary_lon.append(geometry.exterior.coords.xy[0])\n",
    "    elif geometry.geom_type == 'MultiPolygon':\n",
    "        for polygon in geometry:\n",
    "            boundary_lat.append(polygon.exterior.coords.xy[1])\n",
    "            boundary_lon.append(polygon.exterior.coords.xy[0])\n",
    "\n",
    "# Print the extracted latitude and longitude coordinates\n",
    "for lat_us, lon_us in zip(boundary_lat, boundary_lon):\n",
    "    print(\"Latitude:\", lat_us)\n",
    "    print(\"Longitude:\", lon_us)\n",
    "    print()\n",
    "\n",
    "# Use the boundary_lat and boundary_lon as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a820ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Assuming you have the grid_values, grid_lat, and grid_lon arrays\n",
    "# Also assuming you have the US mainland boundary polygon\n",
    "\n",
    "# Iterate over each grid point\n",
    "for i in range(grid_lat.shape[0]):\n",
    "    for j in range(grid_lat.shape[1]):\n",
    "        lat = grid_lat[i, j]\n",
    "        lon = grid_lon[i, j]\n",
    "        \n",
    "        # Create a shapely Point object for the grid point coordinates\n",
    "        point = Point(lon, lat)\n",
    "        \n",
    "        # Check if the point is outside the US boundary\n",
    "        if not us49.boundary.contains(point).any():\n",
    "            grid_values[i, j] = np.nan\n",
    "            grid_lat[i, j] = np.nan\n",
    "            grid_lon[i, j] = np.nan\n",
    "            print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7050084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "us49.boundary.contains(Point(-99,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "us49.boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1624e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
